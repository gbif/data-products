# Identifiying datasets of type (Sample Event)

The overall mission is to enable users to identify datasets most relevant to their purpose. Tagging or categorizing the GBIF datasets will serve as a launchpad for our effort to make this kind of filtering possible. Example: "Give me all records from sediment-corer type samples"  

Initially all datasets in GBIF should be parsed by using a range of keywords that signify a sampling event dataset.
These can be 'plot', 'transect', 'trap', 'trawl'... (of course there are many others). I just used these high level terms for the code prototyping. More specific ones, such as 'malaise trap' will be added along the way.

The Elastic Search query strategy for identifying sampling event datasets changed from making a query with multiple different sampling protocols inside, to a query of one protocol term at the time and its plural/participle siblings.
Example ['survey', 'surveys', 'surveying']  
Elastic Search was chosen because it comes with a range of built in tools that could become useful, such as stemming, lemmatization, and other Natural Language Processing (NLP) tools and techniques.

The repository for these terms is this:
https://docs.google.com/spreadsheets/d/16lEFzwLVBfjONXGflnLpWre_kDnhrRKdU9hPnmqr_M4/edit#gid=2049077323
(To be expanded)


 * The first step is to assemble *sampling protocol terms* which is a large subject unto itself.<br/>We would want members of the community with knowledge in this area to contribute to the Sampling Event vocabulary.
 * Once there is a pool of terms sufficiently diverse and sufficiently broad, then we can throw these at GBIF ElasticSearch : `http://registry-search.gbif.org:9200/dataset/_search/?_source=title,description,samplingDescription.sampling&_source_excludes=geographicCoverages` (only works inside the GBIF firewall) - using Curl or Postman. The JSON body will look similar to what you see here: https://github.com/gbif/data-products/blob/master/sample_event_candidates/ES_multi_match.txt <br/>This JSON payload is generated by the Python code, so the complexity of this is already taken care of.
 * The response from ElasticSearch is JSON formatted and will be parsed into a csv file by Python.<br/>From here it is a question of quickly reviewing the output csv to see if there are false positives.
 * Now the datasets identified can be amended with the sampling protocol terms unique to each dataset.<br/>How exactly GBIF is going to implement this (probably categorizing rather than tagging) is the question. This opens the option of categorizing on record level which is a discussion yet to be had.




# The sample_event_ES_dataset_finder.py module itself

![alt text](https://github.com/gbif/data-products/blob/master/sample_event_candidates/Se_code_function_chart_medium.png)
There is a clean() method in the code to deal with free-text fields in GBIF, which occationally contain impurities such as HTML elements, line breaks, control characters and others.
The methods mining() and regex_check_term() interact to get sample protocol terms assigned to the row under construction. Then that record gets written to a csv file. 

Record:
| dataset_key | title | description | sampling | protocol_terms | ES_score |
| --- | --- | --- | --- | --- | --- |
|85b0b9ce-1fac-46a6-ac25-c4ea3645b7af  | artsprosjektet 46 15 noramph | Project goals in application Inventory of the Norwegian... |Sledges  TRAWLs  grabs  corers  ROVs and scubadivers| 'trawl' | 13.98 |
