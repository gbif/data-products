# Identifiying datasets of type (Sample Event)

The mission is to enable users to identify datasets most relevant to their purpose. Tagging or categorizing the GBIF datasets will serve as a launchpad for our effort to make this kind of filtering possible. Example: "Give me all records from sediment-corer type samples"
Initially all datasets in GBIF should be parsed by using a range of keywords that signify a sampling event dataset.
These can be 'plot', 'transect', 'trap', 'trawl'... (of course there are many others). I just used these for the code prototyping.

The Elastic Search query strategy for identifying sampling event datasets changed from making a query with multiple different sampling protocols inside, to a query of one protocol term at the time and its plural/participle siblings.
Example ['survey', 'surveys', 'surveying']

The repository for these terms is this:
https://docs.google.com/spreadsheets/d/16lEFzwLVBfjONXGflnLpWre_kDnhrRKdU9hPnmqr_M4/edit#gid=2049077323
(To be expanded)


 * The first step is to assemble *sampling protocol terms* which is a large subject unto itself.<br/>We would want members of the community with knowledge in this area to contribute to the Sampling Event vocabulary.
 * Once there is a pool of terms sufficiently diverse and sufficiently broad, then we can throw these at GBIF ElasticSearch : `http://registry-search.gbif.org:9200/dataset/_search/?_source=title,description,samplingDescription.sampling&_source_excludes=geographicCoverages` (only works inside the GBIF firewall) - using Curl or Postman. The JSON body will look similar to what you see here: https://github.com/gbif/data-products/blob/master/sample_event_candidates/ES_multi_match.txt <br/>This JSON payload is generated by the Python code, so the complexity of this is already taken care of.
 * The response from ElasticSearch is JSON formatted and will be parsed into a csv file by Python.<br/>From here it is a question of quickly reviewing the output csv to see if there are false positives.
 * Now the datasets identified can be amended with the sampling protocol terms unique to each dataset.<br/>How exactly GBIF is going to implement this (probably categorizing rather than tagging) is the question. This opens the option of categorizing on record level which is a discussion yet to be had.|




# The sampling_event_elastic_multiterms.py module itself

![alt text](https://github.com/gbif/data-products/blob/master/sample_event_candidates/Se_code_function_chart_medium.png)
The keywords_to_uppercase() method is needed because free-text fields in GBIF occationally contain impurities such as HTML elements, line breaks, control characters and others.
The methods mining() and keywords_to_uppercase() exchange a dictionary structure that gets assigned values until a complete record is formed. Then that record gets written to a csv file. 

Record:
| dataset_key | title | description | sampling | protocol_terms | ES_score |
| --- | --- | --- | --- | --- | --- |
|85b0b9ce-1fac-46a6-ac25-c4ea3645b7af  | artsprosjektet 46 15 noramph | Project goals in application Inventory of the Norwegian... |Sledges  TRAWLs  grabs  corers  ROVs and scubadivers| 'trawl' | 13.98 |

(* ) Protocol_terms being a list of the different terms identified in the metadata text fields.
