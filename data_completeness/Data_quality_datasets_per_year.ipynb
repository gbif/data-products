{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of this script:\n",
    "\n",
    "I want to know:\n",
    "* which datasets recently published (2018) have a lot of incomplete records (here I set the threshold to 100% incomplete records),\n",
    "* and what is the most frequent reason for inocmpleteness.\n",
    "\n",
    "Then I want to aggregate this information per endorsing node and their continent.\n",
    "\n",
    "\n",
    "## First, get the information from hive\n",
    "\n",
    "The definition on incompleteness can be found on any [country page](https://www.gbif.org/country/SE/publishing), it is the following:\n",
    ">A record is here defined to be complete if it includes an identification at least to species rank, valid coordinates, a full date of occurrence and a given basis of record (e.g. Observation, specimen etc).\n",
    "\n",
    "This is the query I ran, it is based on the [following ones](https://github.com/gbif/analytics/blob/master/hive/process/occ_complete_v2.q) (which are used to calculate the number of complete records accross snapshots).\n",
    "\n",
    "I decided not to use the snapshots.\n",
    "\n",
    "This query gives us mor infomration than needed but I thought it would be usedful to have, in case we get asked more details about the record completeness.\n",
    "\n",
    "Unlike for the [queries this was inspired from](https://github.com/gbif/analytics/blob/master/hive/process/occ_complete_v2.q), this is rgouped by datasets instead of by snapshots.\n",
    "\n",
    "``` sql\n",
    "SELECT\n",
    "datasetkey,\n",
    "basisofrecord,\n",
    "COALESCE(\n",
    "    CASE WHEN species IS NOT NULL AND species != taxonid THEN 'Infraspecies' ELSE NULL END,\n",
    "    CASE WHEN species IS NOT NULL THEN 'Species' ELSE NULL END,\n",
    "    CASE WHEN genus IS NOT NULL THEN 'Genus' ELSE NULL END,\n",
    "    CASE WHEN family IS NOT NULL THEN 'Family' ELSE NULL END,\n",
    "    CASE WHEN order_ IS NOT NULL THEN 'Order' ELSE NULL END,\n",
    "    CASE WHEN class IS NOT NULL THEN 'Class' ELSE NULL END,\n",
    "    CASE WHEN phylum IS NOT NULL THEN 'Phylum' ELSE NULL END,\n",
    "    CASE WHEN kingdom IS NOT NULL AND kingdom != 0 THEN 'Kingdom' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  ) AS rank,\n",
    "  COALESCE(\n",
    "    CASE WHEN hascoordinate = TRUE THEN 'Georeferenced' ELSE NULL END,\n",
    "    CASE WHEN countrycode IS NOT NULL THEN 'CountryOnly' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  ) AS geospatial,\n",
    "  COALESCE(\n",
    "    CASE WHEN year IS NOT NULL AND month IS NOT NULL AND day IS NOT NULL THEN 'YearMonthDay' ELSE NULL END,\n",
    "    CASE WHEN year IS NOT NULL AND month IS NOT NULL THEN 'YearMonth' ELSE NULL END,\n",
    "    CASE WHEN year IS NOT NULL THEN 'YearOnly' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  ) AS temporal,\n",
    "  COUNT(*) AS count\n",
    "FROM occurrence_hdfs\n",
    "WHERE to_date(from_unixtime(cast(fragmentcreated/1000 as INT))) >= \"2018-01-01\" AND to_date(from_unixtime(cast(fragmentcreated/1000 as INT))) < \"2019-01-01\"\n",
    "GROUP BY\n",
    "  datasetkey,\n",
    "  basisofrecord,\n",
    "  COALESCE(\n",
    "    CASE WHEN species IS NOT NULL AND species != taxonid THEN 'Infraspecies' ELSE NULL END,\n",
    "    CASE WHEN species IS NOT NULL THEN 'Species' ELSE NULL END,\n",
    "    CASE WHEN genus IS NOT NULL THEN 'Genus' ELSE NULL END,\n",
    "    CASE WHEN family IS NOT NULL THEN 'Family' ELSE NULL END,\n",
    "    CASE WHEN order_ IS NOT NULL THEN 'Order' ELSE NULL END,\n",
    "    CASE WHEN class IS NOT NULL THEN 'Class' ELSE NULL END,\n",
    "    CASE WHEN phylum IS NOT NULL THEN 'Phylum' ELSE NULL END,\n",
    "    CASE WHEN kingdom IS NOT NULL AND kingdom != 0 THEN 'Kingdom' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  ),\n",
    "  COALESCE(\n",
    "    CASE WHEN hascoordinate = TRUE THEN 'Georeferenced' ELSE NULL END,\n",
    "    CASE WHEN countrycode IS NOT NULL THEN 'CountryOnly' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  ),\n",
    "  COALESCE(\n",
    "    CASE WHEN year IS NOT NULL AND month IS NOT NULL AND day IS NOT NULL THEN 'YearMonthDay' ELSE NULL END,\n",
    "    CASE WHEN year IS NOT NULL AND month IS NOT NULL THEN 'YearMonth' ELSE NULL END,\n",
    "    CASE WHEN year IS NOT NULL THEN 'YearOnly' ELSE NULL END,\n",
    "    \"Unknown\"\n",
    "  );```\n",
    "  \n",
    "## Second, get information from the registry\n",
    "\n",
    "This is to be able to summarise the completness information per year of dataset created and per node/continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname='prod_b_registry', \n",
    "                        user='', \n",
    "                        host='pg1.gbif.org', \n",
    "                        password='')\n",
    "cur = conn.cursor()\n",
    "# This is the query for the registry\n",
    "query = \"\"\"SELECT d.key, d.created, node.title, node.continent, node.country\n",
    "           FROM dataset d JOIN organization o ON d.publishing_organization_key = o.key\n",
    "           JOIN node ON o.endorsing_node_key = node.key;\"\"\"\n",
    "# Queries the GBIF registry\n",
    "cur.execute(query)\n",
    "res_query = cur.fetchall()\n",
    "\n",
    "dataset_per_publishing_country = pd.DataFrame(res_query, columns=['UUID',\n",
    "                                                                  'created',\n",
    "                                                                  'nodeTitle',\n",
    "                                                                  'nodeContinent',\n",
    "                                                                  'nodeCountry'])\n",
    "dataset_per_publishing_country = dataset_per_publishing_country.set_index(\"UUID\")\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load information from saved hive table and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_2018_path = \"../2018_datasets_query20190117.csv\"\n",
    "dq_2017_path = \"../2017_datasets_query20190117.csv\"\n",
    "dq_2018 = pd.read_table(dq_2018_path, sep = \",\")\n",
    "dq_2017 = pd.read_table(dq_2017_path, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_name(dq):\n",
    "    return ((dq[\"rank\"] == \"Species\") | (dq[\"rank\"] == \"Infraspecies\"))\n",
    "\n",
    "def complete_basisofrecord(dq):\n",
    "    return dq[\"basisofrecord\"] != \"UNKNOWN\"\n",
    "    \n",
    "def complete_geospatial(dq):\n",
    "    return dq[\"geospatial\"] == \"Georeferenced\"\n",
    "\n",
    "def complete_temporal(dq):\n",
    "    return dq[\"temporal\"] == \"YearMonthDay\"\n",
    "\n",
    "def complete_records(dq_year):\n",
    "    '''\n",
    "    return a mask for a given dataset of complete records\n",
    "    '''\n",
    "    complete = complete_name(dq_year) & complete_basisofrecord(dq_year) & complete_geospatial(dq_year) & complete_temporal(dq_year)\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_complete_records_for_year(dq_year):\n",
    "    '''\n",
    "    Return number of complete and total records published in a given year\n",
    "    in addition to the main issues for incomplete records\n",
    "    '''\n",
    "    records_stats = pd.DataFrame(columns=[\"complete\", \"total\"],\n",
    "                                 index=dq_year.groupby([\"datasetkey\"]).first().index)\n",
    "    complete = complete_records(dq_year)\n",
    "    records_stats[\"complete\"] = dq_year[complete].groupby([\"datasetkey\"])[\"count\"].sum()\n",
    "    records_stats[\"total\"] = dq_year.groupby([\"datasetkey\"])[\"count\"].sum()\n",
    "    records_stats = records_stats.fillna(0)\n",
    "    \n",
    "    # Check which is the most frequent incompleteness reason\n",
    "    reason_incomplete = dq_year[~complete].sort_values([\"datasetkey\", \"count\"],\n",
    "                                                       ascending=False).groupby(\"datasetkey\").first()\n",
    "    records_stats = pd.concat([records_stats, reason_incomplete], axis = 1)\n",
    "    return records_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ds_by_year_registry(dataset_per_publishing_country, year):\n",
    "    '''\n",
    "    return a mask to select datasets created a certain year\n",
    "    '''\n",
    "    before_that_year = pd.to_datetime(dataset_per_publishing_country.created, utc=True) >= datetime.datetime(year=year, month=1, day=1)\n",
    "    after_that_year = pd.to_datetime(dataset_per_publishing_country.created, utc=True) < datetime.datetime(year=year+1, month=1, day=1)\n",
    "    return (before_that_year & after_that_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_for_ds_published_in_year(dq_year, dataset_per_publishing_country, year):\n",
    "    '''\n",
    "    returns datasets filtered for a given year and merged\n",
    "    '''\n",
    "    dq_stats = number_of_complete_records_for_year(dq_year)\n",
    "    mask = select_ds_by_year_registry(dataset_per_publishing_country, year)\n",
    "    dq_ds_stats = pd.concat([dataset_per_publishing_country[mask], dq_stats], axis = 1, join = 'inner')\n",
    "\n",
    "    return dq_ds_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_incompleteness_reason(dataset):\n",
    "    '''\n",
    "    For a given subset, return the most frequent reason\n",
    "    for data incompleteness\n",
    "    '''\n",
    "    res = pd.DataFrame(columns=[\"freq\"])\n",
    "    res.at[\"basis of record\", \"freq\"] = dataset[~complete_basisofrecord(dataset)].shape[0]\n",
    "    res.at[\"taxon rank\", \"freq\"] = dataset[~complete_name(dataset)].shape[0]\n",
    "    res.at[\"geospatial\", \"freq\"] = dataset[~complete_geospatial(dataset)].shape[0]\n",
    "    res.at[\"temporal\", \"freq\"] = dataset[~complete_temporal(dataset)].shape[0]\n",
    "    return res.idxmax()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_stats(dq_year, dataset_per_publishing_country, groupby, threshold, year):\n",
    "    '''\n",
    "    Aggregates statistics per node or continent\n",
    "    '''\n",
    "    # Column titles\n",
    "    percentage_below_thresh = \"Pct of \"+str(year)+\" datasets with incomplete data\"\n",
    "    incompleteness = \"Main reason of incompleteness in \"+str(year)\n",
    "    \n",
    "    # Calculate the number of complete records per dataset\n",
    "    datasets_stats = completeness_for_ds_published_in_year(dq_year, dataset_per_publishing_country, year)\n",
    "    datasets_stats[\"complete_datasets_mask\"] = datasets_stats[\"complete\"]/datasets_stats[\"total\"] <= threshold\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    stats_per_groupby = pd.DataFrame(columns=[percentage_below_thresh])\n",
    "    # Calculate percentage of datasets with incomplete data records\n",
    "    stats_per_groupby[percentage_below_thresh] = datasets_stats.groupby(groupby)[\"complete_datasets_mask\"].sum()*100/datasets_stats.groupby([groupby])[\"complete_datasets_mask\"].count()\n",
    "    # Get most frequent reason for incompleteness\n",
    "    for subgroup in stats_per_groupby.index.tolist():\n",
    "        subset = datasets_stats[datasets_stats[groupby] == subgroup]\n",
    "        stats_per_groupby.at[subgroup, incompleteness] = main_incompleteness_reason(subset)\n",
    "    return stats_per_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_per_continent_2018 = groupby_stats(dq_2018, dataset_per_publishing_country, \"nodeContinent\", 0, 2018)\n",
    "stats_per_continent_2017 = groupby_stats(dq_2018, dataset_per_publishing_country, \"nodeContinent\", 0, 2017)\n",
    "stats_per_node_2018 = groupby_stats(dq_2018, dataset_per_publishing_country, \"nodeTitle\", 0, 2018)\n",
    "stats_per_node_2017 = groupby_stats(dq_2018, dataset_per_publishing_country, \"nodeTitle\", 0, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_per_continent = pd.concat([stats_per_continent_2018, stats_per_continent_2017], axis=1)\n",
    "stats_per_node = pd.concat([stats_per_node_2018, stats_per_node_2017], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('data_quality_for_2018.xlsx')\n",
    "stats_per_continent.to_excel(writer,'CONTINENT')\n",
    "stats_per_node.to_excel(writer,'ENDOSING NODE')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
